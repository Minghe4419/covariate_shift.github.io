---
title: "Kernel Mean Matching"
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(quadprog)
library(ggplot2)
library(proxy)
library(gridExtra)
library(dplyr)
```

## Theoretical Background

- Estimator:

  $\mathbb{E}_{X \sim \mathbb{P}^{(0)}} [\Phi(X)] = \mathbb{E}_{X \sim \mathbb{P}^{(1)}} [\Phi(X)w(X)]$ for any appropriate "anchor" function $\Phi$

- Anchor Function: 

  Consequence of Injection: If $\tilde{w}: \chi \rightarrow \mathbb{R}_{+}$ satisfies $\mathbb{E}_{X \sim \mathbb{P^{(1)}}} \tilde{w}(X) = 1$ and $\mathbb{E}_{X \sim \mathbb{P}^{(0)}} [\Phi(X)] = \mathbb{E}_{X \sim \mathbb{P}^{(1)}} [\Phi(X)\tilde{w}(X)]$, then $\tilde{w} = w$
  
  When $\Phi(x) = K(., x)$ is a feature map of the [Reproducing Kernel Hilbert Space (RKHS)](https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space) induced by a universal kernel $K, \mu: \mathbb{P} \mapsto \mathbb{E}_{X \sim \mathbb{P} \Phi(X)}$ is injective, i.e. $\mathbb{P} \neq \mathbb{Q} \Rightarrow \mu(\mathbb{P}) \neq \mu(\mathbb{Q})$
  
- Practical Algorithm:

  Requires $\{x^{(k)}_{i}\}^{n_k}_{i=1}$ $\overset{\mathrm{iid}}{\sim}$ $\mathbb{P}^{(k)}$, $\epsilon$, $B > 0$ as input. 
  
  Solve $min ||\frac{1}{n_1}\sum_{i=1}^{n_1} w_i \Phi(x_{i}^{(1)}) - \frac{1}{n_0} \sum_{i=1}^{n_0} \Phi(x_{i}^{(0)} ||^{2}_{\mathcal{H}}$
  
  s.t. $w_i \in [0, B], |\frac{1}{n_1} \sum_{i=1}^{n_1}w_i - 1| \leq \epsilon$
  
  By [Reproducing property](https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space), $||\frac{1}{n_1}\sum_{i=1}^{n_1} w_i \Phi(x_{i}^{(1)}) - \frac{1}{n_0} \sum_{i=1}^{n_0} \Phi(x_{i}^{(0)} ||^{2}_{\mathcal{H}}  = \frac{1}{n_{1}^{2}} w^{T} K w - \frac{2}{n_1} {\kappa}^{T}w + constant$, where $\kappa_i = \frac{1}{n_0}\sum_{j=1}{n_0}K(x_{j}^{(0)}, x_{i}^{(1)})$. Hence it becomes a [quadratic programming (QP) problem](https://en.wikipedia.org/wiki/Quadratic_programming).

## Implementation


```{r}
# To install 'densityratio' package, we need libraries:
library(osqp)
library(pbapply)
library(RcppProgress)
#install.packages('densityratio', repos = 'https://thomvolker.r-universe.dev')
library(densityratio)

D <- 20
v <- seq_len(D)  # Example: v = (1, 2, ..., 20)

# Number of samples
n_source <- 1000
n_target <- 1000

# Function to generate mixture of Gaussians
mix_2_gaussian <- function(n, means, sds) {
  comps <- sample.int(2, size = n, replace = TRUE, prob = c(0.5, 0.5))
  rnorm(n, mean = means[comps], sd = sds[comps])
}

# Generate source and target t values
source_t <- mix_2_gaussian(n_source, c(4, 7), c(1.5, 1.5))  # Higher variance
target_t <- mix_2_gaussian(n_target, c(4, 6), c(1, 1))  # Lower variance


X_source <- outer(source_t, v)  # n_source x D
X_target <- outer(target_t, v)  # n_target x D

# Compute true density ratio
dbinorm <- function(x, p, dif1, dif2, sd) {
  p * dnorm(x, dif1, sd) + (1 - p) * dnorm(x, dif2, sd)
}
dratio_kmm <- function(x, p, dif1, dif2, sd1, p2, dif3, dif4, sd2) {
  dbinorm(x, p, dif1, dif2, sd1) / dbinorm(x, p2, dif3, dif4, sd2)
}
```

```{r}

fit_kmm <- kmm(
  df_numerator = target_t, 
  df_denominator = source_t
  )

summary(fit_kmm)
# Predict estimated density ratio
pred_kmm  <- predict(fit_kmm, newdata = source_t)
summary(pred_kmm)

# pred_kmm[pred_kmm<0] <- 0
```


```{r}
ggplot() + 
  geom_density(aes(x = target_t, colour = "target")) +
  geom_density(aes(x = source_t, colour = "source"))

ggplot() +
  geom_density(aes(x = target_t, color = "target")) +
  geom_density(aes(x = source_t, weight = as.numeric(pred_kmm), color = "weighted source"))

# dratio_kmm need adjustment
# ggplot() +
#   geom_point(aes(x = source_t, y = pred_kmm, col = "estimates")) +
#   stat_function(mapping = aes(col = "True density ratio"), 
#                 fun = dratio_kmm, 
#                 args = list(p = 0.5, dif1 = 4, dif2 = 7, sd1 = 1.5, p2 = 0.5, dif3 = 4, dif4 = 6, sd2 = 1),
#                 linewidth = 1)
```


## Highlights

- For definition of *kernel*, *universal kernel*, etc; refer back to our [slide](https://www.columbia.edu/~yt2661/STL/slides/Lecture-3.pdf).

- KMM does not perform well in real-world data.

## References

- Wikipedia

## Code Appendix

Maximum Mean Discrepancy(MMD) can give a direct measure of how well Kernel Mean Matching (KMM) has aligned the mean embeddings in the feature spaceâ€”even if the raw (weighted) distributions look different in input space.

```{r}
# compute_mmd2_weighted <- function(X_source, w_opt, X_target, kernel_func, ...) {
#   n_s <- nrow(X_source)
#   n_t <- nrow(X_target)
#   
#   # Convert w -> alpha, i.e. alpha_i = w_i / n_s
#   alpha <- w_opt / n_s
#   
#   # For target, each sample has weight = 1/n_t
#   beta <- rep(1 / n_t, n_t)
#   
#   # We'll compute the 3 sums in the MMD^2 formula:
#   # sum_{i,i'} alpha_i alpha_i' K(x_i, x_i')
#   # sum_{j,j'} beta_j beta_j' K(y_j, y_j')
#   # sum_{i,j} alpha_i beta_j K(x_i, y_j)
#   
#   # 1) Weighted source-source part
#   ss_sum <- 0
#   for (i in 1:n_s) {
#     for (i2 in 1:n_s) {
#       ss_sum <- ss_sum + alpha[i] * alpha[i2] * kernel_func(X_source[i,], X_source[i2,], ...)
#     }
#   }
#   
#   # 2) Weighted target-target part
#   tt_sum <- 0
#   for (j in 1:n_t) {
#     for (j2 in 1:n_t) {
#       tt_sum <- tt_sum + beta[j] * beta[j2] * kernel_func(X_target[j,], X_target[j2,], ...)
#     }
#   }
#   
#   # 3) Cross term: sum_{i,j} alpha_i beta_j K(x_i, y_j)
#   st_sum <- 0
#   for (i in 1:n_s) {
#     for (j in 1:n_t) {
#       st_sum <- st_sum + alpha[i] * beta[j] * kernel_func(X_source[i,], X_target[j,], ...)
#     }
#   }
#   
#   # MMD^2 = ss_sum + tt_sum - 2 * st_sum
#   mmd2 <- ss_sum + tt_sum - 2 * st_sum
#   return(mmd2)
# }
# 
# linear_kernel <- function(x, y) {
#   sum(x * y)
# }
# 
# mmd2_val <- compute_mmd2_weighted(
#   X_source, w_opt, X_target,
#   kernel_func = linear_kernel
# )
# 
# cat("MMD^2 between weighted source & target = ", mmd2_val, "\n")
# cat("MMD = ", sqrt(mmd2_val), "\n")
```
