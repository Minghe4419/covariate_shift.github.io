---
title: "Histogram-based Method"
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(MASS)
library(dplyr)
library(gridExtra)
library(mvtnorm)
library(tidyverse)
```

## Theoretical Background

### Estimator

- Goal: estimating $w = \frac{f^{Target}(X)}{f^{Source}(X)}$ from $\{x_{i^{(k)}}\}^{n_k}_{i=1} \overset{\mathrm{iid}}{\sim} f^{(k)}$

- Estimator[(Kpotufe, 2017)](https://proceedings.mlr.press/v54/kpotufe17a/kpotufe17a.pdf): $\tilde{w} = \frac{\hat{\mathbb{P}}^{(T)}(B(x, r))}{\hat{\mathbb{P}}^{(S)}(B(x, r))} = \frac{n_{0}^{-1}\sum_{i=1}^{n_0}\mathbb{1}(||x - x_{i}^{(T)}||_2 \leq r)}{n_{1}^{-1}\sum_{i=1}^{n_1}\mathbb{1}(||x - x_{i}^{(S)}||_2 \leq r}$; $\hat{w}(x) = \tilde{w}(x) \mathbb{1}(\hat{\mathbb{P}}^{(S)}(B(x, r)) \geq \alpha)$

- Optimal radius $r$ is found by a data driven algorithm in [(Kpotufe, 2017)](https://proceedings.mlr.press/v54/kpotufe17a/kpotufe17a.pdf) Theorem 3.4.1

### Assumptions

- Compact Support: $supp(\mathbb{P}^{(1)})$ is compact in $\mathbb{R}^{d}$

- Bounded Density Ratio: $||f||_{\infty} \leq B < \infty$

- $\beta$-Hölder class: $|w(x) - w(x')| \leq L||x - x'||_{2}^{\beta}$ with $\beta \in (0, 1]$

## Implementation

```{r}
library(ggplot2)

# Example synthetic data
set.seed(123)
source_data <- matrix(rnorm(100 * 2, mean = 0), ncol = 2)
target_data <- matrix(rnorm(50 * 2, mean = 1), ncol = 2)

# Convert to data frames
source_df <- data.frame(x = source_data[,1], y = source_data[,2])
target_df <- data.frame(x = target_data[,1], y = target_data[,2])

# Query point and radius
x_query <- c(1, 2)
r <- 0.4

# Compute distances
dist_source <- sqrt(rowSums((source_data - matrix(x_query, nrow = nrow(source_data), ncol = 2, byrow = TRUE))^2))
dist_target <- sqrt(rowSums((target_data - matrix(x_query, nrow = nrow(target_data), ncol = 2, byrow = TRUE))^2))

# Circle coordinates
theta <- seq(0, 2 * pi, length.out = 200)
circle <- data.frame(
  x = x_query[1] + r * cos(theta),
  y = x_query[2] + r * sin(theta)
)

# Plot with fixed aspect ratio
ggplot() +
  geom_point(data = source_df, aes(x = x, y = y), color = 'blue', alpha = 0.6) +
  geom_point(data = target_df, aes(x = x, y = y), color = 'red', alpha = 0.6) +
  geom_point(aes(x = x_query[1], y = x_query[2]), shape = 4, size = 5, stroke = 2) +
  geom_path(data = circle, aes(x = x, y = y), linetype = "dashed") +
  coord_fixed() + 
  theme_minimal() +
  labs(title = "w(X) = number of red dots / number of blue dots", subtitle = "Red = Target, Blue = Source",
       x = "Feature 1", y = "Feature 2")
```

```{r}
# ----------------------------
# 1) Setup
# ----------------------------
set.seed(123)  # for reproducibility

D <- 20
v <- seq_len(D)  # v = (1, 2, ..., 20)

n_source <- 1000
n_target <- 400

# ----------------------------
# 2) Define mixture distributions
# ----------------------------
# Q (target): t ~ 0.5*N(-4, 1) + 0.5*N(4, 1)
# P (source): t ~ 0.5*N(-4, 4) + 0.5*N(4, 4)

# A helper function to sample from a 2-component mixture
# with equal weights 0.5 and 0.5, given:
#   means = c(m1, m2), sds = c(sd1, sd2), and total n
mix_2_gaussian <- function(n, means, sds) {
  # Step 1: Decide which component each point comes from
  comps <- sample.int(2, size = n, replace = TRUE, prob = c(0.5, 0.5))
  
  # Step 2: For each chosen component, draw from corresponding Normal
  rnorm(n, mean = means[comps], sd = sds[comps])
}

# ----------------------------
# 3) Generate the scalars t
# ----------------------------
target_t <- mix_2_gaussian(
  n    = n_target,
  means = c(-2, 4),
  sds   = c(1, 1)
)

source_t <- mix_2_gaussian(
  n    = n_source,
  means = c(-4, 4),
  sds   = c(2, 2)  # because variance=4 => sd=2
)

# ----------------------------
# 4) Embed each scalar in D-dimensional space: X = t * v
# ----------------------------
X_target <- outer(target_t, v)  # n_target x D matrix
X_source <- outer(source_t, v)  # n_source x D matrix

# Optionally, convert to data frames:
df_target <- as.data.frame(X_target)
df_source <- as.data.frame(X_source)

# Now you have (X_source, X_target) with dimension 20,
# each lying on the line spanned by v (since x = t*v).

f_target <- function(t) {
  0.5 * dnorm(t, mean = -2, sd = 1) +
  0.5 * dnorm(t, mean = 4,  sd = 1)
}

f_source <- function(t) {
  0.5 * dnorm(t, mean = -4, sd = 2) +
  0.5 * dnorm(t, mean = 4,  sd = 2)
}

# True weight function w(t)
w_fun <- function(t) {
  f_target(t) / f_source(t)
}


```

We generate multi-Gaussian distribution to simulate non-parametric data. Dimension of data $d$ is `r D`; sample size of target data $n_0$ is `r n_target`; and sample size of source data $n_1$ is `r n_source`.

All the parameters we use are from [Kpotufe(2017)](https://proceedings.mlr.press/v54/kpotufe17a/kpotufe17a.pdf).

```{r simulate_estimation}
compute_histogram_ratio <- function(x, target_data, source_data, r) {
  # x is a numeric vector of length d
  # target_data: n0 x d
  # source_data: n1 x d
  # r: numeric radius
  
  n0 <- nrow(target_data)
  n1 <- nrow(source_data)
  
  # Indicator sum for target_data
  # 1/ if distance <= r, 0/ otherwise
  # We'll compute Euclidean distance from x to each row in target_data
  dist_target <- apply(target_data, 1, function(xi) sqrt(sum((xi - x)^2)))
  indicators_target <- (dist_target <= r)
  numerator <- mean(indicators_target)  # (1/n0) sum_i 1(...)
  
  # Indicator sum for source_data
  dist_source <- apply(source_data, 1, function(xi) sqrt(sum((xi - x)^2)))
  indicators_source <- (dist_source <= r)
  denominator <- mean(indicators_source)  # (1/n1) sum_j 1(...)
  
  # Avoid division by zero if the denominator = 0
  if (denominator == 0) {
    ratio <- Inf  # or define a small offset to avoid Inf, e.g. ratio <- numerator / (denominator + 1e-10)
  } else {
    ratio <- numerator / denominator
  }
  
  # To obtain the re-weighted test vector
  alpha = log(n0) / n0
  estimate_reweight <- ifelse(
    denominator >= alpha, ratio, 0
  )
  
  list(ratio = ratio, 
       estimation = estimate_reweight)
}

target_sample_n = 1000
source_sample_n = 1000
min_sample = min(target_sample_n, source_sample_n)
# Choose parameters
r = 45
test_n = 1000
set.seed(335)

test_t <- mix_2_gaussian(
  n     = test_n,
  means = c(-4, 4),
  sds   = c(2, 2)
)

# Embed in D dimensions
X_test <- outer(test_t, v)
test_estimations <- matrix(NA, nrow = test_n, ncol = 2)
weighted_ratio   <- numeric(test_n)

for (i in seq_len(test_n)) {
  # x_test is 1 x D
  x_test <- X_test[i, ]   # a single test sample
  
  # Compute histogram ratio
  hist_ratio <- compute_histogram_ratio(x_test, X_target, X_source, r)
  weighted_ratio[i]         <- hist_ratio$ratio
  test_estimations[i, 1]    <- hist_ratio$estimation
  test_estimations[i, 2]    <- x_test[1] / v[1]  # store the 1st coordinate, or any coordinate
}

estimations_df <- data.frame(
  estimation = test_estimations[, 1],
  t          = test_estimations[, 2]
)
```

```{r}
# Visualize estimation result

# source dataframe
df_t_w_source <- data.frame(
  t = source_t,
  w = w_fun(source_t),
  f = f_source(source_t)
)

# target dataframe
target_f <- data.frame(
  t = target_t,
  f = f_target(target_t)
) 

source_target <- ggplot() +
  geom_density(data = df_target, aes(x = V1, color = "Target")) +
  geom_density(data = df_source, aes(x = V1, color = "Source")) +
  scale_color_manual(
    name   = "distribution",  # this is the legend title
    values = c("Target" = "red",
               "Source" = "blue")
  ) +
  labs(x = "t", y = "Density", title = "Target vs Source Distribution") +
  theme_minimal() +
  theme(
    legend.title = element_text(size = 6),  # Smaller legend title
    legend.text  = element_text(size = 6)    # Smaller legend text
  )

estimation_true <- ggplot() +
  geom_density(data = df_t_w_source, aes(x = t, weight = w, color = "Weighted Source")) +
  geom_density(data = target_f, aes(x = t, color = "Target")) +
  scale_color_manual(
    name = "Distribution",
    values = c("Weighted Source" = "blue", "Target" = "red")
  ) +
  labs(
    x = "t",
    y = "Density",
    title = "Weighted Source vs. Target Distribution"
  ) +
  theme_minimal() +
  theme(
    legend.title = element_text(size = 6),  # Smaller legend title
    legend.text  = element_text(size = 6)    # Smaller legend text
  )


grid.arrange(source_target, estimation_true, ncol = 2)
```

# ICU Patient Example

```{r}
dat <- read_csv("./data/data_forSDE.csv")

source_data <- dat %>%
  filter(mechvent == 1, los < 150) %>%
  select(los) %>%
  drop_na()

target_data <- dat %>%
  filter(mechvent == 0, los < 150) %>%
  select(los) %>%
  drop_na()

compute_histogram_ratio <- function(x, target_data, source_data, r) {
  # x is a numeric vector of length d
  # target_data: n0 x d
  # source_data: n1 x d
  # r: numeric radius
  
  n0 <- nrow(target_data)
  n1 <- nrow(source_data)
  
  # Indicator sum for target_data
  # 1/ if distance <= r, 0/ otherwise
  # We'll compute Euclidean distance from x to each row in target_data
  dist_target <- apply(target_data, 1, function(xi) sqrt(sum((xi - x)^2)))
  indicators_target <- (dist_target <= r)
  numerator <- mean(indicators_target)  # (1/n0) sum_i 1(...)
  
  # Indicator sum for source_data
  dist_source <- apply(source_data, 1, function(xi) sqrt(sum((xi - x)^2)))
  indicators_source <- (dist_source <= r)
  denominator <- mean(indicators_source)  # (1/n1) sum_j 1(...)
  
  # Avoid division by zero if the denominator = 0
  if (denominator == 0) {
    ratio <- Inf  # or define a small offset to avoid Inf, e.g. ratio <- numerator / (denominator + 1e-10)
  } else {
    ratio <- numerator / denominator
  }
  
  # To obtain the re-weighted test vector
  alpha = log(n0) / n0
  estimate_reweight <- ifelse(
    denominator >= alpha, ratio, 0
  )
  
  list(ratio = ratio, 
       estimation = estimate_reweight)
}

X_source <- source_data$los
X_target <- target_data$los
target_n <- length(X_target)
target_estimations <- matrix(NA, nrow = target_n, ncol = 2)
weighted_ratio   <- numeric(target_n)


r <- 0.5  # Example value; adjust as needed

# Loop through each test point (x in target data)
for (i in seq_len(target_n)) {
  x_i <- X_target[i]
  
  # Convert to numeric vector of length d (if your data is 1D like los, this is fine)
  res <- compute_histogram_ratio(x = x_i, 
                                 target_data = as.matrix(X_target), 
                                 source_data = as.matrix(X_source), 
                                 r = r)
  
  # Store both raw ratio and estimation (based on alpha thresholding)
  target_estimations[i, ] <- c(res$ratio, res$estimation)
  weighted_ratio[i]     <- res$estimation
}
```

```{r}
df_original <- bind_rows(
  tibble(value = X_source, group = "Source(mechvent=1)"),
  tibble(value = X_target, group = "Target(mechvent=0)")
)

ggplot(df_original, aes(x = value, color = group)) +
  geom_density(size = 1.2) +
  labs(title = "Original Density: Source vs Target",
       x = "LOS", y = "Density") +
  theme_minimal()

# 2. Weighted source vs target
# We'll sample from source using weights (with replacement)
# Make sure weighted_ratio is aligned with source points
# Here we assume you want to visualize how reweighting changes the *source* to match the *target*

# Note: If weighted_ratio is for each target sample, we need to compute weights for *each source point*.
# So let's reverse it: for each point in source, estimate its importance weight based on target.

# Recompute: weight each source point using the same method (target: target_data, source: source_data)
compute_weights_for_source <- function(source_data, target_data, r = 0.5) {
  n <- nrow(source_data)
  weights <- numeric(n)
  for (i in 1:n) {
    x_i <- source_data[i, ]
    res <- compute_histogram_ratio(
      x = x_i,
      target_data = target_data,
      source_data = source_data,
      r = r
    )
    weights[i] <- res$estimation
  }
  return(weights)
}

# Assuming los is a 1D variable
source_matrix <- matrix(X_source, ncol = 1)
target_matrix <- matrix(X_target, ncol = 1)
weights_source <- compute_weights_for_source(source_matrix, target_matrix, r = 0.5)

# Create weighted source dataframe
df_weighted <- tibble(
  value = X_source,
  weights = weights_source
)

# Remove zero weights (optional, for better density estimation)
df_weighted <- df_weighted %>% filter(weights > 0)

# Plot weighted density vs target
ggplot() +
  geom_density(data = tibble(value = X_target), aes(x = value), color = "red", size = 1.2, linetype = "dashed") +
  geom_density(data = df_weighted, aes(x = value, weight = weights), color = "blue", size = 1.2) +
  labs(title = "Weighted Source vs Target Density",
       x = "LOS", y = "Density",
       caption = "Red dashed: Target | Blue: Weighted Source") +
  theme_minimal()
```
```

## Highlights

- Histogram-based method allow us to estimate the target data's distribution with source data whether the data is parametric or not(same as Kernel Density Estimation). But it also suffers from the curse of dimensionality/multivariate data.

## References

[Kpotufe, S. (2017). Lipschitz density-ratios, structured data, and data-driven tuning. In Artificial Intelligence and Statistics, pages 1320–1328. PMLR.](https://proceedings.mlr.press/v54/kpotufe17a.html)
